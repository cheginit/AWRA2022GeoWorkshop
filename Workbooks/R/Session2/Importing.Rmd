---
title: "Importing Spatial Data"
author: "Marc Weber"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_notebook:
    toc: yes
    toc_float: yes
    toc_depth: 4
  html_document:
    toc: yes
    keep_md: true
    toc_depth: '4'
    df_print: paged
editor_options:
  chunk_output_type: inline
---
## Setup
```{r}
library(sf)
library(ggplot2)
library(dplyr)
library(readr)
library(knitr)
library(rnaturalearth)
library(stringr)
library(osmdata)
library(mapview)
library(remotes)
# install_github("mhweber/awra2020spatial")
library(awra2020spatial)
sessionInfo()
```

## Lesson 1: Accessing Spatial Data Files
- There are several ways we typically get spatial data into R:
  + Load spatial files we have on our machine or from remote source
  + Load spatial data that is part of an R package
  + Grab data using API (often making use of particular R packages)
  + Converting flat files with x,y data to spatial data
  + Geocoding data

### Vector Data
For reading and writing vector data in R, the primary package you'll use is
 `sf`for vector formats such as [ESRI Shapefiles](https://en.wikipedia.org/wiki/Shapefile),  [GeoJSON](https://en.wikipedia.org/wiki/GeoJSON), and [GPX](https://en.wikipedia.org/wiki/GPS_Exchange_Format) - `sf` uses **OGR**, which is a library under the [GDAL](https://en.wikipedia.org/wiki/GDAL) source tree,under the hood

We can quickly discover supported I/O vector formats with `sf`:
```{r message=FALSE, warning=FALSE, error=FALSE}
print(paste0('There are ',st_drivers("vector") %>% nrow(), ' vector drivers available using st_read or read_sf'))
```
`sf` can be used to read numerous file types:

- Shapefiles
- Geodatabases
- Geopackages
- Geojson
- Spatial database files

#### Shapefiles
Typically working with vector GIS data we work with ESRI shapefiles or geodatabases - here we have an example of how one would read in a shapefile using `sf`:
```{r}
# download.file("ftp://ftp.gis.oregon.gov/adminbound/citylim_2017.zip","citylim_2017.zip")
# unzip("citylim_2017.zip", exdir = ".")
citylims <- st_read("citylim_2017.shp")
```

```{r}
options(scipen=3)
ggplot(citylims) + geom_sf() + ggtitle('Oregon City Limits')
```

#### Geodatabases
```{r}
# download.file("ftp://ftp.gis.oregon.gov/adminbound/OregonStateParks_20181010.zip", "OregonStateParks.zip")
# unzip("OregonStateParks.zip", exdir = ".")
fgdb = "OregonStateParks_20181010.gdb"

# List all feature classes in a file geodatabase
st_layers(fgdb)
```

Read in the layer
```{r}
# Read the feature class
parks <- st_read(dsn=fgdb,layer="LO_PARKS")
ggplot(parks) + geom_sf()
```

#### Geopackages
Here we read in a geopackage loaded as part of the `sf` package.  We do a couple things here - in `st_layers`, we see there is just one layer, so we don't need to specify `layer` as a parameter in `read_sf`.  Second, we use pipes and dplyr syntax to read in our file - a nice background resource that delves into using the `tidyverse` is [R for Data Science](https://r4ds.had.co.nz/index.html).

```{r}
st_layers(system.file("gpkg/nc.gpkg", package="sf"))
nc <- system.file("gpkg/nc.gpkg", package="sf") %>% read_sf() # reads in
glimpse(nc)
write_sf(nc, 'nc.gpkg', 'nc', delete_layer = TRUE)
```

#### Exercise
What are a couple advantages of `geopackages` over `shapefiles`?

Some thoughts [here](https://towardsdatascience.com/why-you-need-to-use-geopackage-files-instead-of-shapefile-or-geojson-7cb24fe56416), main ones probably:

- geopackages avoid mult-file format of shapefiles
- geopackages avoid the 2gb limit of shapefiles
- geopackages are open-source and follow OGC standards
- lighter in file size than shapefiles
- geopackages avoid the 10-character limit to column headers in shapefile attribute tables (stored in archaic .dbf files)

#### Open spatial data sources
There's a wealth of open spatial data accessible online now via static URLs or APIs - just a few examples include [Data.gov](https://catalog.data.gov/dataset?metadata_type=geospatial), NASA [SECAC Portal](http://sedac.ciesin.columbia.edu/), [Natural Earth](http://www.naturalearthdata.com/), [UNEP GEOdata](http://geodata.grid.unep.ch/), and countless others listed here at [Free GIS Data](https://freegisdata.rtwilson.com/)

Below is an example of pulling in US states using the `rnaturalearth` package - note that the default is to pull in data as `sp` objects and we coerce to `sf`.  Also take a look at the chained operation using dplyr.  Try changing the filter or a parameter in ggplot.
```{r naturalearth, , message=FALSE, warning=FALSE, error=FALSE}
states <- ne_states(country = 'United States of America')
states_sf <- st_as_sf(states)
states_sf %>%
  dplyr::filter(!name %in% c('Hawaii','Alaska') & !is.na(name)) %>%
  ggplot + geom_sf()
```

```{r}
states <- read_sf('https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_500k.json')
states <- states[!states$NAME %in% c('Hawaii','Alaska'),]
plot(states$geometry)
```

#### Reading in subsets of vector data
Imagine we have really large datasets we need to work with.  We can subset the data as we read-in using SQL with `sf` as in this toy example using states data above.  Let's try to read in just PNW states from the states .json data.  See [st_read](https://r-spatial.github.io/sf/reference/st_read.html) for another example of this using the `sf` package `nc` data.
```{r}
query_str <- str_c('SELECT * FROM "gz_2010_us_040_00_500k" WHERE NAME IN (\'','Oregon', '\', \'','Washington','\',\'','Idaho','\')')
states <- read_sf('https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_500k.json',query = query_str)
glimpse(states)
plot(states$`_ogr_geometry_`)
```

#### Read in OpenStreetMap data
The [osmdata](https://github.com/ropensci/osmdata) package is facilitates accesses the OpenStreetMap (OSM) database in R and allows us to pull OSM data directly into R data structures and work with in R.

First we'll find available tags to get foot paths to plot
```{r osmdata1 , message=FALSE, warning=FALSE, error=FALSE}
head(available_tags("highway")) # get rid of head when you run - just used to truncate output
```

```{r osmdata2 , message=FALSE, warning=FALSE, error=FALSE}
footway <- opq(bbox = "corvallis oregon") %>%
  add_osm_feature(key = "highway", value = c("footway","cycleway","path", "path","pedestrian","track")) %>%
  osmdata_sf()
footway <- footway$osm_lines
rstrnts <- opq(bbox = "corvallis oregon") %>%
    add_osm_feature(key = "amenity", value = "restaurant") %>%
    osmdata_sf()
rstrnts <- rstrnts$osm_points
mapview(footway$geometry) + mapview(rstrnts)
```

#### Exercise
Take a minute and try pulling in data of your own for your own area and plotting using `osmdata`

#### Spatial subset
We can also perform a spatial subset on read-in.  We extract our geometry list-column as well-known text and pass it as a filter on read in.
```{r}
wkt = st_as_text(st_geometry(states[1,])) # here we grab just WA from our PNW states in previous step
# filter by bbox of first feature geometry selected above:
states <- read_sf('https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_500k.json',wkt_filter = wkt)
plot(states$geometry)
# why are Oregon and Idaho still included?
```

#### Exercise
Based on examples of reading in different datasets and subsetting, try to find a dataset online to read in, or try a different subset operation on example datasets so far, or both.

### Raster Data
For reading and writing raster data in R, the primary packages you'll use are:
 * `raster`, `terra`, or `stars` for raster formats such as [GeoTIFF](https://en.wikipedia.org/wiki/GeoTIFF) or [ESRI or ASCII grid](https://en.wikipedia.org/wiki/Esri_grid) using [GDAL](https://en.wikipedia.org/wiki/GDAL) under the hood.

`raster` has been superceeded by `terra` so I will only show examples of `terra` here, and `stars` provides additional funcionality for dealing with spatio-temporal arrays, is `tidyvere`-compliant and works with `sf`, so I prefer it in many ways to `terra`.

We can get a list of all `gdal` drivers available via `sf`:
```{r message=FALSE, warning=FALSE, error=FALSE}
print(paste0('There are ',st_drivers(what='raster') %>% nrow(), ' raster drivers available'))
kable(head(st_drivers(what='raster'),n=5))
```

#### `terra` package
Load stock elevation .tif file that comes with package
```{r terra_hist, message=FALSE, warning=FALSE, error=FALSE}
library(terra)
f <- system.file("ex/elev.tif", package="terra")
elev <- rast(f)
barplot(elev, digits=-1, las=2, ylab="Frequency")
```

```{r terra_plot, message=FALSE, warning=FALSE, error=FALSE}
plot(elev)
```

#### `stars` package
Load stock Landsat 7 .tif file that comes with package
```{r stars_read, message=FALSE, warning=FALSE, error=FALSE}
library(stars)
tif = system.file("tif/L7_ETMs.tif", package = "stars")
read_stars(tif) %>%
  dplyr::slice(index = 1, along = "band") %>%
  plot()
```

We'll get a sense for what 'slice' and 'index' above are doing when we get into geoprocessing.

### Convert flat files to spatial
We often have flat files, locally on our machine or accessed elsewhere, that have coordinate information which we would like to make spatial.

In the steps below, we

  1. read in a .csv file of USGS gages in the PNW that have coordinate columns
  2. Use `st_as_sf` function in `sf` to convert the data frame to an `sf` spatial simple feature collection by:
      a. passing the coordinate columns to the coords parameter
      b. specifying a coordinate reference system (CRS)
      c. opting to retain the coordinate columns as attribute columns in the resulting `sf` feature collection.
  3. Keep only the coordinates and station ID in resulting `sf` feature collection, and
  4. Plotting our gages as spatial features with `ggplot2` using `geom_sf`.

```{r vector_drivers, message=FALSE, warning=FALSE, error=FALSE}
gages = read_csv(system.file("extdata/Gages_flowdata.csv", package = "Rspatialworkshop"))
gages_sf <- gages %>%
  st_as_sf(coords = c("LON_SITE", "LAT_SITE"), crs = 4269, remove = FALSE) %>%
  dplyr::select(STATION_NM,LON_SITE, LAT_SITE)
ggplot() + geom_sf(data=gages_sf)
```
